{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Monte Carlo (MC) uncertainty propagation\n",
    "\n",
    "MC error propagation can be very useful when a derivation for a given uncertainty is too complicated to calculate, when one is too lazy to derive many functions, or simply when the operator does not feel comfortable enough anymore to derive an uncertainty propagation without mathematical errors. MC error propagation - compared to regular error propagation - furthermroe does not intrinsically assume that an uncertainty of a given measurement around its mean value is normally distributed, actually any distribution can be assumed.  \n",
    "In this introduction however we will assume that everything follows a Gaussian distribution. Let us however have a look on how to generate random numbers and how to test the random number generator in python\n",
    "\n",
    "## Random numbers\n",
    "MC methods depend heavily on radnom number generators. However, a computer cannot generate true random number and uses pseudo random number generators. In python, we can use the random package that is distributed with numpy. Let us have a look on how to use this package and see if these numbers look - at least optically - randomly distributed.\n",
    "\n",
    "### Random numbers between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "0.1896510177011853\n"
     ]
    }
   ],
   "source": [
    "import numpy as np   # of course we should import the NumPy package\n",
    "import matplotlib.pyplot as plt   # to optically check if things are randomly distributed\n",
    "%pylab ipympl\n",
    "# generate a random number between 0 and 1 and print it\n",
    "print(np.random.rand())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "243b4ad6bed94b2b9bc0f16cc2488315",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create 100000 random numbers and plot them in a histogram of 100 bins. we should see pretty much\n",
    "# equal bin size of 100 bins with 100 entries each\n",
    "rn = np.random.rand(10000)\n",
    "\n",
    "# make the histogram\n",
    "plt.figure(1)\n",
    "rn_hist = plt.hist(rn, bins=100)   # we are also going to save the histogram for further use...\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can print and run this histogram over and over again and will see that numbers are pretty equally distributed. Give it a try and plug in more random numbers. How high do you need to go, i.e., how many random numbers do you need to generate until the histogram looks pretty much flat?\n",
    "\n",
    "Let us now have a look on what the mean and the standard deviation of this random number package looks like. Note: here we are checking how many entries the bins have on average (and not what the mean of the distribution is). We can use the associated numpy functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average: 100.000, Standard deviation: 9.505\n"
     ]
    }
   ],
   "source": [
    "rn_avg = np.mean(rn_hist[0])\n",
    "rn_std = np.std(rn_hist[0])\n",
    "print(f\"Average: {rn_avg:.3f}, Standard deviation: {rn_std:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks pretty good, the average is where we expect it and the standard deviation makes sense too. We can also look at the mean of the distribution and its standard deviation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average: 0.499, Standard deviation: 0.290\n"
     ]
    }
   ],
   "source": [
    "print(f\"Average: {np.mean(rn):.3f}, Standard deviation: {np.std(rn):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks as expected as well. So the random number generator seems to work. \n",
    "\n",
    "### Gaussian distributed random numbers\n",
    "For a given measurement we usually don't assume that the uncerainty is distributed with equal chances. We expect a Gaussian distribution. NumPy has a function for this as well. Let us create 10000 samples that are Gaussian distributed around 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f97ab808c04045059485edf008b573ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gn = np.random.randn(10000)   # Note: randn instead of rand - stands for 'random normal'\n",
    "\n",
    "# plot a histogram\n",
    "plt.figure(2)\n",
    "gn_hist = plt.hist(gn, bins=100)\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks pretty Gaussian at first glance, so it seems to work. Let's have a look at the mean and standard deviation of all the values this time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average of distribution: 0.002, Standard deviation: 0.988\n"
     ]
    }
   ],
   "source": [
    "gn_avg = np.mean(gn)\n",
    "gn_std = np.std(gn)\n",
    "print(f\"Average of distribution: {gn_avg:.3f}, Standard deviation: {gn_std:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the average of the distribution is about zero. The standard deviation - big surprise - is about 1. This is expected, since this distribution is exactly used to define the standard deviation.\n",
    "\n",
    "So far so good, our random number generators seem to work. Now let's define an experimental setup that we want to analyze. Let's choose a simple system that we can easily solve numerically as well.\n",
    "\n",
    "## Experiment setup and analytical evaluation\n",
    "Let us assume a radioactively decaying species. We measure the number of atoms at the beginning $N_0$ at the number of atoms after a given time t as $N(t)$. The radioactive decay equation with decay constant $\\lambda$ can then be written as:\n",
    "\n",
    "$$N(t) = N_0 \\exp{(-\\lambda t)}$$\n",
    "\n",
    "Assuming we want to measure the decay constant $\\lambda$, we can solve above equation for $\\lambda$:\n",
    "\n",
    "$$\\lambda = \\frac{1}{t} \\ln \\left(\\frac{N(t)}{N_0}\\right)$$\n",
    "Of course, every measurement has an uncertainty. Let us define the uncertainty in the time measurement $t$ as $\\sigma_t$, the uncertainty in $N(t)$ as $\\sigma_N$ and the uncertainty in $N_0$ as $\\sigma_0$. The derivatives of $\\lambda$ for all the variablies with uncertainties are then:\n",
    "\n",
    "$$\\frac{\\partial \\lambda}{\\partial t} = \\frac{1 - \\ln \\left(\\frac{N(t)}{N_0}\\right)}{t^{2}}$$\n",
    "$$\\frac{\\partial \\lambda}{\\partial N(t)} = \\frac{1}{tN(t)}$$\n",
    "$$\\frac{\\partial \\lambda}{\\partial N_0} = -\\frac{1}{tN_0}$$\n",
    "\n",
    "The propagated uncertainty of $\\lambda$ can then be written as:\n",
    "\n",
    "$$ \\sigma_{\\lambda} =  \\left\\{ \\left[ \\frac{1 - \\ln \\left(\\frac{N(t)}{N_0}\\right)}{t^{2}} \\sigma_t \\right]^2 + \\left[ \\frac{1}{tN(t)} \\sigma_N \\right]^2 + \\left[ -\\frac{1}{tN_0} \\sigma_0 \\right]^2 \\right\\}^{\\frac{1}{2}}  $$\n",
    "\n",
    "Let us set up some random measurement variables and calculate the decay constant and its uncertainty from it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decay constant: lambda = -0.00385\n",
      "Uncertainty of lambda: sigma_lambda = 0.00079\n",
      "Uncertainty of lambda: sigma_lambda = 20.4%\n"
     ]
    }
   ],
   "source": [
    "# Measurements:\n",
    "nt = 50.\n",
    "n0 = 100.\n",
    "tmsr = 180.\n",
    "sigma_nt = 5.\n",
    "sigma_n0 = 10.\n",
    "sigma_t = 1.\n",
    "\n",
    "# Calculate lambda:\n",
    "lam = 1. / tmsr * np.log(nt / n0)\n",
    "print(f\"Decay constant: lambda = {lam:.5f}\")\n",
    "\n",
    "# Calculate uncertainty:\n",
    "sigma_lam = (((1 - np.log(nt/n0)) * sigma_t / tmsr**2.)**2. + (sigma_nt / (tmsr * nt))**2. + \n",
    "             (-sigma_n0 / (tmsr*n0))**2.)**0.5\n",
    "print(f\"Uncertainty of lambda: sigma_lambda = {sigma_lam:.5f}\")\n",
    "print(f\"Uncertainty of lambda: sigma_lambda = {np.abs(sigma_lam / lam) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure, after all the derivations, this can be calculated, however, the chances of making a mistake can be rather high. We can evaluate the same uncertainty on $\\lambda$ using MC error propagation. \n",
    "\n",
    "### MC uncertainty propoagation of the setup\n",
    "\n",
    "Let us first define how many samples we want to consider for the MC setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples for MC setup\n",
    "nint = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can calculate the value of $\\lambda$ as many times as we have samples. Each time, the measurement uncertainties for $N(t)$, $N_0$, and $t$ can be varied randomly by adding the the standard deviation times a random number drawn from a gaussian distribution with standard deviation 1 to the respective value. \n",
    "\n",
    "Here we are using the same variables as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate lambda samples -> this is a np.array with nint entries!\n",
    "lamsamp = 1. / (tmsr + np.random.randn(nint) * sigma_t) * np.log((nt + np.random.randn(nint) * sigma_nt) / \n",
    "                                                                 (n0 + np.random.randn(nint) * sigma_n0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this looks also very complicated, it is actually fairly straight forward. You should first notice, that the equation is the same as above, where we calculated lambda. The only thing in addition is the following code, e.g., for $t$:\n",
    "    \n",
    "    tmsr + np.random.randn(nint) * sigma_t\n",
    "\n",
    "Here we simply take a randomly drawn number from a Gaussian distribution and multiply it with the uncertainty of $t$, e.g., $\\sigma_t$. The value that we effectively plug into the equation for $t$ is on average exactly what we expect $t$ to be, a gaussian distributed envelope around the mean value $t$ with standard deviation $\\sigma_t$. \n",
    "\n",
    "This process is now repeated for every measurement, e.g.:\n",
    "\n",
    "    nt + np.random.randn(nint) * sigma_nt\n",
    "    n0 + np.random.randn(nint) * sigma_n0\n",
    "    \n",
    "**Attention:** Do NOT predefine the random number and then use the same random number to multiply with all uncertainties. This will induce highly correlated uncertainties into your system, which is something that you don't want (of course, unless the randomness is correlated to each other).\n",
    "\n",
    "*Note:* Of course you do not have to choose from a Gaussian distribution, in fact, you can plug in any distribution that you would like! This is one of the huge advantages of MC error propoagation.\n",
    "\n",
    "Let us know evaluate and calculate the uncertainty for lambda, i.e., we want to calculate the average decay constant, and its associated uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MC calculation of lambda and the associated uncertainty:\n",
      "Average decay constant: lambda = -0.00386\n",
      "Uncertainty of lambda: sigma_lambda = 0.00080\n",
      "Uncertainty of lambda: sigma_lambda = 20.8%\n"
     ]
    }
   ],
   "source": [
    "print('MC calculation of lambda and the associated uncertainty:')\n",
    "lamavg = np.average(lamsamp)\n",
    "lamstd = np.std(lamsamp)\n",
    "print(f'Average decay constant: lambda = {lamavg:.5f}')\n",
    "print(f'Uncertainty of lambda: sigma_lambda = {lamstd:.5f}')\n",
    "print(f'Uncertainty of lambda: sigma_lambda = {np.abs(lamstd / lamavg * 100.):.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These values, as you can see, agree very well with the propagated uncertaintes that we calculated above.\n",
    "\n",
    "### Convergence of the MC error propagation\n",
    "\n",
    "An important part of MC error propagation is to look for convergence, i.e., how good is the calculation if we repeat it over and over again. In order to test this, we can run the above calculation multiple times and then look at the standard deviation of the average value and average uncertainty that we calculate. To do so, let us first put above calculation into a subroutine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mccalc():\n",
    "    lamsamp = 1. / (tmsr + np.random.randn(nint) * sigma_t) * np.log((nt + np.random.randn(nint) * sigma_nt) / \n",
    "                                                                     (n0 + np.random.randn(nint) * sigma_n0))\n",
    "    # calculate average and standard deviation\n",
    "    avg = np.average(lamsamp)\n",
    "    std = np.std(lamsamp)\n",
    "    \n",
    "    # return the average and the standard deviation\n",
    "    return avg, std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can set up a test vector and run the MC calculation, e.g., 1000 times. Then we can define how much variation the current number of samples (10000) produce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average of lambda calculated by MC is +/- : 0.2%\n",
      "The standard deviation of lambda calculated by MC is +/- : 0.7%\n"
     ]
    }
   ],
   "source": [
    "nconv = 1000\n",
    "# define the average vector and std vector\n",
    "vecavg = np.zeros(nconv)\n",
    "vecstd = np.zeros(nconv)\n",
    "\n",
    "# now we can run the subroutine nconv times and add the average and standard deviation to the respective cell in \n",
    "# vecavg and vecstd\n",
    "for it in range(nconv):\n",
    "    avgtmp, stdtmp = mccalc()\n",
    "    vecavg[it] = avgtmp\n",
    "    vecstd[it] = stdtmp\n",
    "    \n",
    "# let us know calculate the standard deviation of these two vectors and print them out in percent deviation from the \n",
    "# actual mean that is calculated\n",
    "vecavgstdpercent = np.abs(np.std(vecavg) / np.mean(vecavg)) * 100.\n",
    "vecstdstdpercent = np.std(vecstd) / np.mean(vecstd) * 100.\n",
    "\n",
    "# print the results\n",
    "print(f'The average of lambda calculated by MC is +/- : {vecavgstdpercent:.1f}%')\n",
    "print(f'The standard deviation of lambda calculated by MC is +/- : {vecstdstdpercent:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These deviations are much smaller than the error itself, and thus perfectly justfied. \n",
    "\n",
    "*Note:* Every MC error propagation should contain a convergence test, i.e., you will need to test how well the MC error propagation works and if you have chosen enough samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    " 1. Write a python subroutine to go through above MC error propagation, however, this time give `nint` as a variable to the subroutine. Go through the same calculation as above and print out the uncertainties when choosing `nint` values of 10, 100, 100000.\n",
    " \n",
    " 2. Using the subroutine written for exercise one, study the convergence of the system as function of `nint`. Create a figure with a logarithmic y axis where you plot `avg` and `std` as returned by your new subroutine `mccalc(nint)` as a function of `nint`. Use an `nconv` of 1000. Discuss your results.\n",
    " \n",
    "   *Hint:* The following routines might be useful for your evaluation:  \n",
    "   `plt.semilogx(...)`: Plots a semilogarithmic figure  \n",
    "   `np.logspace(startexp, stopexp, numb)`: Creates a logarithmically spaced array that starts at '10**(startexp)', ends at '10**(stopexp)' and contains 'numb' entries\n",
    "\n",
    " 3. So far, you have tried to constrain convergence with `nconv=1000` samples. Decrease and increase this number and discuss your findings. Was nconv = 1000 a good estimate? Why?\n",
    "  Hint: Look at the figure in exercise 2. From this figure you can see when nint is converged. It might be a good idea to use this nint to study the convergence of nconv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
